{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d46c7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from typing import Dict, Iterable, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "BASE = os.getenv(\"OPENALEX_BASE\", \"https://api.openalex.org\")\n",
    "CONTACT = os.getenv(\"CONTACT_EMAIL\", \"mhmoslemi2338@gmail.com\")\n",
    "\n",
    "def paginate(endpoint: str, params: Dict[str, Any], max_pages: int) -> Iterable[Dict]:\n",
    "    params = dict(params)\n",
    "    if CONTACT:\n",
    "        params[\"mailto\"] = CONTACT\n",
    "    url = f\"{BASE}/{endpoint}\"\n",
    "    cursor = \"*\"\n",
    "    for _ in range(max_pages):\n",
    "        qp = dict(params)\n",
    "        qp[\"cursor\"] = cursor\n",
    "        r = requests.get(url, params=qp, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "        for item in js.get(\"results\", []):\n",
    "            yield item\n",
    "        cursor = js.get(\"meta\", {}).get(\"next_cursor\")\n",
    "        if not cursor:\n",
    "            break\n",
    "        time.sleep(0.5)  # rate limit politeness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11c639e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b329485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Works shape: (100000, 6)\n",
      "Authorships shape: (538529, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['work_id', 'title', 'year', 'venue', 'cited_by_count', 'source_issn_l'], dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load parquet files\n",
    "works = pd.read_parquet(\"data/curated/works.parquet\")\n",
    "auths = pd.read_parquet(\"data/curated/authorships.parquet\")\n",
    "\n",
    "print(\"Works shape:\", works.shape)\n",
    "print(\"Authorships shape:\", auths.shape)\n",
    "\n",
    "# Merge them on work_id to get a combined view\n",
    "# combined = auths.merge(works, on=\"work_id\", how=\"left\")\n",
    "\n",
    "# print(\"Combined shape:\", combined.shape)\n",
    "\n",
    "works.columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b5aa62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebb91e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: works.parquet  (100000 rows)\n",
      "Saved: authorships.parquet  (538410 rows)\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/Users/mohammad/Downloads/citation-bias-explorer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 211>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m wout \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp/works.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    210\u001b[0m aout \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp/authorships.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 211\u001b[0m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maout\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(input_path, works_out, authorships_out)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(input_path: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m    186\u001b[0m             works_out: Union[\u001b[38;5;28mstr\u001b[39m, Path] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworks.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    187\u001b[0m             authorships_out: Union[\u001b[38;5;28mstr\u001b[39m, Path] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthorships.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m     works_raw \u001b[38;5;241m=\u001b[39m \u001b[43m_read_works\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     works_rows \u001b[38;5;241m=\u001b[39m [_flatten_work(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m works_raw]\n\u001b[1;32m    191\u001b[0m     auth_rows \u001b[38;5;241m=\u001b[39m [row \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m works_raw \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m _flatten_authorships(w)]\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m_read_works\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m\"\"\"Read OpenAlex works from:\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m   - JSON array file\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m   - single JSON object\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m   - concatenated JSON objects in a single file.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m p \u001b[38;5;241m=\u001b[39m Path(path)\n\u001b[0;32m---> 45\u001b[0m raw \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Try JSON array or single JSON object\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.23/Frameworks/Python.framework/Versions/3.9/lib/python3.9/pathlib.py:1194\u001b[0m, in \u001b[0;36mPath.read_text\u001b[0;34m(self, encoding, errors)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    Open the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m   1195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.23/Frameworks/Python.framework/Versions/3.9/lib/python3.9/pathlib.py:1180\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, buffering\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1175\u001b[0m          errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1176\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;124;03m    Open the file pointed by this path and return a file object, as\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;124;03m    the built-in open() function does.\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mopener\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/Users/mohammad/Downloads/citation-bias-explorer'"
     ]
    }
   ],
   "source": [
    "# save as openalex_to_parquet.py\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Union\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _brace_split(s: str) -> List[str]:\n",
    "    \"\"\"Split concatenated JSON objects by brace counting (handles embedded strings/escapes).\"\"\"\n",
    "    objs, depth, start = [], 0, None\n",
    "    in_str = False\n",
    "    esc = False\n",
    "    for i, ch in enumerate(s):\n",
    "        if in_str:\n",
    "            if esc:\n",
    "                esc = False\n",
    "            elif ch == \"\\\\\":\n",
    "                esc = True\n",
    "            elif ch == '\"':\n",
    "                in_str = False\n",
    "        else:\n",
    "            if ch == '\"':\n",
    "                in_str = True\n",
    "            elif ch == \"{\":\n",
    "                if depth == 0:\n",
    "                    start = i\n",
    "                depth += 1\n",
    "            elif ch == \"}\":\n",
    "                depth -= 1\n",
    "                if depth == 0 and start is not None:\n",
    "                    objs.append(s[start : i + 1])\n",
    "                    start = None\n",
    "    return objs\n",
    "\n",
    "\n",
    "def _read_works(path: Union[str, Path]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Read OpenAlex works from:\n",
    "       - JSON array file\n",
    "       - single JSON object\n",
    "       - concatenated JSON objects in a single file.\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    raw = p.read_text(encoding=\"utf-8\").strip()\n",
    "\n",
    "    # Try JSON array or single JSON object\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "        if isinstance(data, dict):  # single work\n",
    "            return [data]\n",
    "        if isinstance(data, list):\n",
    "            return data\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: concatenated objects\n",
    "    objs = _brace_split(raw)\n",
    "    if not objs:\n",
    "        raise ValueError(\"Could not parse input as JSON array/object or concatenated JSON objects.\")\n",
    "    out = []\n",
    "    for obj in objs:\n",
    "        out.append(json.loads(obj))\n",
    "    return out\n",
    "\n",
    "\n",
    "def _jdumps(x: Any) -> str:\n",
    "    return json.dumps(x, ensure_ascii=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _flatten_work(w: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    pl = (w.get(\"primary_location\") or {}) or {}\n",
    "    src = (pl.get(\"source\") or {}) or {}\n",
    "    oa  = (w.get(\"open_access\") or {}) or {}\n",
    "    return {\n",
    "        # core\n",
    "        \"work_id\": w.get(\"id\"),\n",
    "        \"title\": w.get(\"title\"),\n",
    "        \"publication_year\": w.get(\"publication_year\"),\n",
    "        \"cited_by_count\": w.get(\"cited_by_count\"),\n",
    "        # primary location\n",
    "        # \"pl_is_oa\": pl.get(\"is_oa\"),\n",
    "        # \"pl_landing_page_url\": pl.get(\"landing_page_url\"),\n",
    "        # \"pl_pdf_url\": pl.get(\"pdf_url\"),\n",
    "        # \"pl_version\": pl.get(\"version\"),\n",
    "        # \"pl_is_accepted\": pl.get(\"is_accepted\"),\n",
    "        # \"pl_is_published\": pl.get(\"is_published\"),\n",
    "        # source\n",
    "        \"source_id\": src.get(\"id\"),\n",
    "        \"source_display_name\": src.get(\"display_name\"),\n",
    "        \"source_type\": src.get(\"type\"),\n",
    "        \"source_issn_l\": src.get(\"issn_l\"),\n",
    "        # \"source_issn\": _jdumps(src.get(\"issn\", [])),\n",
    "        # \"source_is_oa\": src.get(\"is_oa\"),\n",
    "        # \"source_is_in_doaj\": src.get(\"is_in_doaj\"),\n",
    "        # \"source_is_indexed_in_scopus\": src.get(\"is_indexed_in_scopus\"),\n",
    "        # \"source_is_core\": src.get(\"is_core\"),\n",
    "        # \"source_host_org\": src.get(\"host_organization\"),\n",
    "        # \"source_host_org_name\": src.get(\"host_organization_name\"),\n",
    "        # \"source_host_org_lineage\": _jdumps(src.get(\"host_organization_lineage\", [])),\n",
    "        # \"source_host_org_lineage_names\": _jdumps(src.get(\"host_organization_lineage_names\", [])),\n",
    "        # OA summary\n",
    "        # \"oa_is_oa\": oa.get(\"is_oa\"),\n",
    "        # \"oa_status\": oa.get(\"oa_status\"),\n",
    "        # \"oa_url\": oa.get(\"oa_url\"),\n",
    "        # \"oa_any_repository_has_fulltext\": oa.get(\"any_repository_has_fulltext\"),\n",
    "        # raw blobs (so nothing is lost)\n",
    "        # \"raw_primary_location\": _jdumps(pl),\n",
    "        # \"raw_open_access\": _jdumps(oa),\n",
    "    }\n",
    "\n",
    "\n",
    "# def _flatten_authorships(w: Dict[str, Any]) -> Iterable[Dict[str, Any]]:\n",
    "#     auths = (w.get(\"authorships\") or []) or []\n",
    "#     for a in auths:\n",
    "#         author = (a.get(\"author\") or {}) or {}\n",
    "#         yield {\n",
    "#             \"work_id\": w.get(\"id\"),\n",
    "#             \"work_title\": w.get(\"title\"),\n",
    "#             \"author_position\": a.get(\"author_position\"),\n",
    "#             # \"is_corresponding\": a.get(\"is_corresponding\"),\n",
    "#             \"raw_author_name\": a.get(\"raw_author_name\"),\n",
    "#             \"raw_affiliation_strings\": \"; \".join((a.get(\"raw_affiliation_strings\") or []) or []),\n",
    "#             \"author_id\": author.get(\"id\"),\n",
    "#             \"author_display_name\": author.get(\"display_name\"),\n",
    "#             # \"author_orcid\": author.get(\"orcid\"),\n",
    "#             \"countries\": \"; \".join((a.get(\"countries\") or []) or []),\n",
    "#             # preserve full affiliation data as JSON\n",
    "#             \"institutions_json\": _jdumps(a.get(\"institutions\", [])),\n",
    "#             \"affiliations_json\": _jdumps(a.get(\"affiliations\", [])),\n",
    "#         }\n",
    "def _flatten_authorships(w: Dict[str, Any]) -> Iterable[Dict[str, Any]]:\n",
    "    auths = (w.get(\"authorships\") or []) or []\n",
    "    for a in auths:\n",
    "        author = (a.get(\"author\") or {}) or {}\n",
    "\n",
    "        # Extract institution names\n",
    "        institutions = a.get(\"institutions\", []) or []\n",
    "        institution_names = \"; \".join([inst.get(\"display_name\", \"\") for inst in institutions if inst.get(\"display_name\")])\n",
    "\n",
    "        # Extract affiliation strings\n",
    "        affils = a.get(\"affiliations\", []) or []\n",
    "        affiliation_strings = \"; \".join([aff.get(\"raw_affiliation_string\", \"\") for aff in affils if aff.get(\"raw_affiliation_string\")])\n",
    "\n",
    "        yield {\n",
    "            \"work_id\": w.get(\"id\"),\n",
    "            \"work_title\": w.get(\"title\"),\n",
    "            \"author_id\": author.get(\"id\"),\n",
    "            \"author_display_name\": author.get(\"display_name\"),\n",
    "            \"author_position\": a.get(\"author_position\"),\n",
    "            \"is_corresponding\": a.get(\"is_corresponding\"),\n",
    "            # \"raw_author_name\": a.get(\"raw_author_name\"),\n",
    "            # \"author_orcid\": author.get(\"orcid\"),\n",
    "            \"countries\": \"; \".join((a.get(\"countries\") or []) or []),\n",
    "            # new flattened fields\n",
    "            \"institution_names\": institution_names,\n",
    "            \"affiliation_strings\": affiliation_strings,\n",
    "            # keep raw JSON in case\n",
    "            # \"institutions_json\": json.dumps(institutions, ensure_ascii=False),\n",
    "            # \"affiliations_json\": json.dumps(affils, ensure_ascii=False),\n",
    "        }\n",
    "\n",
    "\n",
    "def write_parquet(df: pd.DataFrame, path: Union[str, Path]) -> None:\n",
    "    # Try pyarrow, then fastparquet\n",
    "    last_err = None\n",
    "    for eng in (\"pyarrow\", \"fastparquet\"):\n",
    "        try:\n",
    "            df.to_parquet(path, index=False, engine=eng)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise RuntimeError(\n",
    "        \"Failed to write Parquet. Install one of the engines:\\n\"\n",
    "        \"  pip install pyarrow\\n\"\n",
    "        \"  # or\\n\"\n",
    "        \"  pip install fastparquet\\n\"\n",
    "        f\"Last error: {last_err}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def convert(input_path: Union[str, Path],\n",
    "            works_out: Union[str, Path] = \"works.parquet\",\n",
    "            authorships_out: Union[str, Path] = \"authorships.parquet\") -> None:\n",
    "    works_raw = _read_works(input_path)\n",
    "\n",
    "    works_rows = [_flatten_work(w) for w in works_raw]\n",
    "    auth_rows = [row for w in works_raw for row in _flatten_authorships(w)]\n",
    "\n",
    "    works_df = pd.DataFrame(works_rows)\n",
    "    auth_df = pd.DataFrame(auth_rows)\n",
    "\n",
    "    write_parquet(works_df, works_out)\n",
    "    write_parquet(auth_df, authorships_out)\n",
    "    print(f\"Saved: {works_out}  ({len(works_df)} rows)\")\n",
    "    print(f\"Saved: {authorships_out}  ({len(auth_df)} rows)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inp = '/Users/mohammad/Downloads/citation-bias-explorer/data/raw/works.jsonl'\n",
    "    wout = sys.argv[2] if len(sys.argv) >= 3 else \"works.parquet\"\n",
    "    aout = sys.argv[3] if len(sys.argv) >= 4 else \"authorships.parquet\"\n",
    "    convert(inp, wout, aout)\n",
    "\n",
    "inp = '/Users/mohammad/Downloads/citation-bias-explorer'\n",
    "wout = \"tmp/works.parquet\"\n",
    "aout = \"tmp/authorships.parquet\"\n",
    "convert(inp, wout, aout)\n",
    "\n",
    "\n",
    "\n",
    "# python openalex_to_parquet.py input.json works.parquet authorships.parquet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a764db28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Works shape: (100000, 12)\n",
      "Authorships shape: (538529, 9)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load parquet files\n",
    "works = pd.read_parquet(\"data/curated/works.parquet\")\n",
    "auths = pd.read_parquet(\"data/curated/authorships.parquet\")\n",
    "\n",
    "print(\"Works shape:\", works.shape)\n",
    "print(\"Authorships shape:\", auths.shape)\n",
    "\n",
    "# Merge them on work_id to get a combined view\n",
    "# combined = auths.merge(works, on=\"work_id\", how=\"left\")\n",
    "\n",
    "# print(\"Combined shape:\", combined.shape)\n",
    "\n",
    "# auths.head().to_csv('tmp/works.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b07e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd3728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11b47f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paginate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mpaginate\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworks\u001b[39m\u001b[38;5;124m\"\u001b[39m, params\u001b[38;5;241m=\u001b[39mparams, max_pages\u001b[38;5;241m=\u001b[39mmax_pages)):\n\u001b[1;32m     33\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps(obj, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m         count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'paginate' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "out_path = \"data/raw/works.jsonl\"\n",
    "from_year = os.getenv(\"FROM_YEAR\")\n",
    "per_page = int(os.getenv(\"WORKS_PER_PAGE\"))\n",
    "max_pages = int(os.getenv(\"WORKS_MAX_PAGES\"))\n",
    "filed = (os.getenv(\"CONCEPT_ID_FILED\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"filter\": f\"concepts.id:{filed},from_publication_date:{from_year}-01-01,type_crossref:journal-article\",\n",
    "    \"per_page\": per_page,\n",
    "    \"select\": \",\".join([\n",
    "        \"id\",\"title\",\"authorships\",\n",
    "        \"primary_location\",\"cited_by_count\",\n",
    "        \"publication_year\",\"open_access\"\n",
    "    ])\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "count = 0\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for obj in tqdm(paginate(\"works\", params=params, max_pages=max_pages)):\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "        count += 1\n",
    "print(f\"Saved {count} works to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36171af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, \n",
      "1 page done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m qp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(params)\n\u001b[1;32m     18\u001b[0m qp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcursor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m cursor\n\u001b[0;32m---> 19\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     21\u001b[0m js \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/api.py:75\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    524\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m'\u001b[39m: allow_redirects,\n\u001b[1;32m    527\u001b[0m }\n\u001b[1;32m    528\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 529\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py:687\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 687\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/models.py:838\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/models.py:760\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 760\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    761\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/urllib3/response.py:579\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 579\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/urllib3/response.py:522\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     cache_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    524\u001b[0m         amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data\n\u001b[1;32m    525\u001b[0m     ):  \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "endpoint = \"works\"\n",
    "CONTACT = os.getenv(\"CONTACT_EMAIL\", \"mhmoslemi2338@gmail.com\")\n",
    "BASE = os.getenv(\"OPENALEX_BASE\", \"https://api.openalex.org\")\n",
    "\n",
    "import time\n",
    "i=0\n",
    "\n",
    "params = dict(params)\n",
    "if CONTACT:\n",
    "    params[\"mailto\"] = CONTACT\n",
    "url = f\"{BASE}/{endpoint}\"\n",
    "cursor = \"*\"\n",
    "for _ in range(max_pages):\n",
    "    i+=1\n",
    "    qp = dict(params)\n",
    "    qp[\"cursor\"] = cursor\n",
    "    r = requests.get(url, params=qp, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    j=0\n",
    "    for item in js.get(\"results\", []):\n",
    "        # print(item)\n",
    "        # continue\n",
    "        j+=1\n",
    "        print(j,end = ', ')\n",
    "    print()\n",
    "    cursor = js.get(\"meta\", {}).get(\"next_cursor\")\n",
    "    if not cursor:\n",
    "        break\n",
    "    time.sleep(0.5)  # rate limit politeness\n",
    "    print(i,'page done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08e6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filter': 'concepts.id:C41008148,from_publication_date:2020-01-01,type_crossref:journal-article',\n",
       " 'per_page': 200,\n",
       " 'select': 'id,title,authorships,primary_location,cited_by_count,publication_year,open_access',\n",
       " 'mailto': 'mhmoslemi2338@gmail.com',\n",
       " 'cursor': 'IlsxMDAuMCwgMjAzMiwgJ2h0dHBzOi8vb3BlbmFsZXgub3JnL1c0Mzc4MjEyNTQ0J10i'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de54d045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total works available: 9822607\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "def get_total_count(params):\n",
    "    url = f\"{BASE}/{endpoint}\"\n",
    "    \n",
    "    r = requests.get(url, params=qp, timeout=60)\n",
    "\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"meta\"][\"count\"]\n",
    "\n",
    "\n",
    "\n",
    "total = get_total_count(params)\n",
    "print(f\"Total works available: {total}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d1dc00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c9a1293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-4.0.1.tar.gz (434.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.2/434.2 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.9\n",
      "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.0/203.0 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-4.0.1-py2.py3-none-any.whl size=434813815 sha256=9c18653f7b2ad40fc628b964c59ed8e5c4f13af9fdb30b7523ee810c986c40c0\n",
      "  Stored in directory: /Users/mohammad/Library/Caches/pip/wheels/10/e6/6b/c50eb601fa827dd56a5272db5d5db360e559e527a80a665b1d\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.9 pyspark-4.0.1\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.types import *\n",
    "!pip3 install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4dd21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56a5a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "works_schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"authorships\", ArrayType(StructType([\n",
    "        StructField(\"author\", StructType([\n",
    "            StructField(\"id\", StringType()),\n",
    "            StructField(\"display_name\", StringType())\n",
    "        ])),\n",
    "        StructField(\"institutions\", ArrayType(StructType([\n",
    "            StructField(\"id\", StringType()),\n",
    "            StructField(\"display_name\", StringType()),\n",
    "            StructField(\"country_code\", StringType())\n",
    "        ]))),\n",
    "        StructField(\"author_position\", StringType())\n",
    "    ]))),\n",
    "    StructField(\"host_venue\", StructType([\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"display_name\", StringType()),\n",
    "        StructField(\"publisher\", StringType())\n",
    "    ])),\n",
    "    StructField(\"cited_by_count\", IntegerType()),\n",
    "    StructField(\"publication_year\", IntegerType()),\n",
    "    StructField(\"open_access\", StructType([\n",
    "        StructField(\"is_oa\", BooleanType())\n",
    "    ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c41c144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/16 20:20:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/09/16 20:20:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "\n",
    "def load_raw_works(spark):\n",
    "    return spark.read.schema(works_schema).json(\"data/raw/works.jsonl\")\n",
    "\n",
    "def curate_works(df):\n",
    "    return df.select(\n",
    "        col(\"id\").alias(\"work_id\"),\n",
    "        col(\"title\"),\n",
    "        col(\"publication_year\").alias(\"year\"),\n",
    "        col(\"host_venue.display_name\").alias(\"venue\"),\n",
    "        col(\"cited_by_count\"),\n",
    "        col(\"open_access.is_oa\").alias(\"is_oa\")\n",
    "    )\n",
    "\n",
    "def flatten_authorships(df):\n",
    "    a = df.select(\n",
    "        col(\"id\").alias(\"work_id\"),\n",
    "        explode(\"authorships\").alias(\"auth\")\n",
    "    )\n",
    "    a = a.select(\n",
    "        \"work_id\",\n",
    "        col(\"auth.author.id\").alias(\"author_id\"),\n",
    "        col(\"auth.author.display_name\").alias(\"author_name\"),\n",
    "        col(\"auth.author_position\").alias(\"author_pos\"),\n",
    "        explode(\"auth.institutions\").alias(\"inst\")\n",
    "    )\n",
    "    a = a.select(\n",
    "        \"work_id\",\"author_id\",\"author_name\",\"author_pos\",\n",
    "        col(\"inst.id\").alias(\"institution_id\"),\n",
    "        col(\"inst.display_name\").alias(\"institution_name\"),\n",
    "        col(\"inst.country_code\").alias(\"country_code\"),\n",
    "    )\n",
    "    return a\n",
    "\n",
    "spark = SparkSession.builder.appName(\"citation_bias_etl\").getOrCreate()\n",
    "df_raw = load_raw_works(spark).dropna(subset=[\"id\"])\n",
    "works = curate_works(df_raw)\n",
    "auth = flatten_authorships(df_raw)\n",
    "\n",
    "os.makedirs(\"data/curated\", exist_ok=True)\n",
    "works.write.mode(\"overwrite\").parquet(\"data/curated/works.parquet\")\n",
    "auth.write.mode(\"overwrite\").parquet(\"data/curated/authorships.parquet\")\n",
    "print(\"ETL complete.\")\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e71303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/java-8-openjdk-amd64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"JAVA_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086ccad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/openjdk@17/17.0.16/libexec/openjdk.jdk/Contents/Home\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"JAVA_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a88e403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b669c112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/16 20:19:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(spark.version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
